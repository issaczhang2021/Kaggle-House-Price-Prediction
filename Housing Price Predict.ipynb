{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc744c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaggle competitions download -c house-prices-advanced-regression-techniques -p \"/Users/issacsmacbookpro/.cursor-tutor/projects/Housing Price Prediction\"\n",
    "# https://www.kaggle.com/code/ryannolan1/kaggle-housing-youtube-video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f89efd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.compose import make_column_transformer, ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler, OneHotEncoder\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor, StackingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f6a5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/Users/issacsmacbookpro/.cursor-tutor/projects/Housing Price Prediction/house-prices-advanced-regression-techniques/train.csv')\n",
    "test_df = pd.read_csv('/Users/issacsmacbookpro/.cursor-tutor/projects/Housing Price Prediction/house-prices-advanced-regression-techniques/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab8c42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add mkt_cyc column to train_df\n",
    "train_df['mkt_cyc'] = pd.cut(train_df['YrSold'],\n",
    "                              bins=[2005, 2008, 2010, 2012],  # Adjusted bins\n",
    "                              labels=['Peak and Initial Decline', 'Crash and Bottom', 'Early Recovery'],  # Corresponding labels\n",
    "                              right=False)  # Changed to right=False to include the left edge\n",
    "\n",
    "# Add mkt_cyc column to test_df\n",
    "test_df['mkt_cyc'] = pd.cut(test_df['YrSold'],\n",
    "                            bins=[2005, 2008, 2010, 2012],  # Adjusted bins\n",
    "                            labels=['Peak and Initial Decline', 'Crash and Bottom', 'Early Recovery'],  # Corresponding labels\n",
    "                            right=False)  # Changed to right=False to include the left edge\n",
    "\n",
    "# Add house_age column to train_df\n",
    "train_df['house_age'] = train_df['YrSold'] - train_df['YearBuilt']  # Calculate house age\n",
    "\n",
    "# Add house_age_range column to train_df\n",
    "train_df['house_age_range'] = pd.cut(\n",
    "    train_df['house_age'],\n",
    "    bins=[-1, 5, 20, 50, 80, 100, float('inf')],\n",
    "    labels=['New Construction', 'Recent Builds', 'Contemporary', 'Mid-Century', 'Pre-War', 'Historic']\n",
    ")\n",
    "\n",
    "# Add house_age column to test_df\n",
    "test_df['house_age'] = test_df['YrSold'] - test_df['YearBuilt']  # Calculate house age\n",
    "\n",
    "# Add house_age_range column to test_df\n",
    "test_df['house_age_range'] = pd.cut(\n",
    "    test_df['house_age'],\n",
    "    bins=[-1, 5, 20, 50, 80, 100, float('inf')],\n",
    "    labels=['New Construction', 'Recent Builds', 'Contemporary', 'Mid-Century', 'Pre-War', 'Historic']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff59807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.head()\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219cbeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming df is your dataframe\n",
    "numeric_features = train_df.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "# Display the numeric features\n",
    "print(numeric_features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d9147f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming df is your dataframe\n",
    "numeric_features_test = test_df.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "# Display the numeric features\n",
    "print(numeric_features_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a95b048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary statistics of numeric features\n",
    "numeric_summary = numeric_features.describe()\n",
    "\n",
    "# Display the summary\n",
    "print(numeric_summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad5ab1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary statistics of numeric features\n",
    "numeric_summary_test = numeric_features_test.describe()\n",
    "\n",
    "# Display the summary\n",
    "print(numeric_summary_test)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba1ab8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in numeric features\n",
    "missing_values = numeric_features.isnull().sum()\n",
    "\n",
    "# Display features with missing values\n",
    "print(missing_values[missing_values > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b3f122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in numeric features\n",
    "missing_values_test = numeric_features_test.isnull().sum()\n",
    "\n",
    "# Display features with missing values\n",
    "print(missing_values_test[missing_values_test > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1740fd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing LotFrontage based on the median LotFrontage for each Neighborhood\n",
    "train_df['LotFrontage'] = train_df.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "train_df['MasVnrArea'] = train_df.groupby('Neighborhood')['MasVnrArea'].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "train_df['GarageYrBlt'] = train_df.groupby('Neighborhood')['GarageYrBlt'].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "# Double Check for missing values in numeric features \n",
    "numeric_features = train_df.select_dtypes(include=['int64', 'float64'])\n",
    "# Check for missing values in numeric features\n",
    "missing_values = numeric_features.isnull().sum()\n",
    "\n",
    "# Display features with missing values\n",
    "print(missing_values[missing_values > 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c0fec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in test_df based on the median for specified features\n",
    "features_to_fill = ['LotFrontage', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', \n",
    "                    'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', \n",
    "                    'BsmtHalfBath', 'GarageYrBlt', 'GarageCars', 'GarageArea']\n",
    "\n",
    "for feature in features_to_fill:\n",
    "    test_df[feature] = test_df[feature].fillna(test_df[feature].median())\n",
    "\n",
    "# Double Check for missing values in numeric features \n",
    "numeric_features_test = test_df.select_dtypes(include=['int64', 'float64'])\n",
    "# Check for missing values in numeric features\n",
    "missing_values_test = numeric_features_test.isnull().sum()\n",
    "\n",
    "# Display features with missing values\n",
    "print(missing_values_test[missing_values_test > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65a5de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Assuming df is your dataframe\n",
    "# Step 1: Sort out numeric features\n",
    "numeric_features = train_df.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "# Step 2: Exclude 'LotArea', 'SalePrice', and 'Id'\n",
    "numeric_features_excluded = numeric_features.drop(columns=['SalePrice', 'Id'], errors='ignore')\n",
    "\n",
    "# Step 3: Create an empty figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Step 4: Add a scatter plot for each feature and make them invisible initially\n",
    "for feature in numeric_features_excluded.columns:\n",
    "    fig.add_trace(go.Scatter(x=train_df[feature], y=train_df['SalePrice'], mode='markers', name=feature, visible=False))\n",
    "\n",
    "# Step 5: Make the first feature visible\n",
    "fig.data[0].visible = True\n",
    "\n",
    "# Step 6: Create slider steps\n",
    "slider_steps = [\n",
    "    {'label': feature, 'method': 'update', 'args': [{'visible': [i == j for j in range(len(fig.data))]}, {'title': f'{feature} vs SalePrice', 'xaxis': {'title': feature}}]}\n",
    "    for i, feature in enumerate(numeric_features_excluded.columns)\n",
    "]\n",
    "\n",
    "# Step 7: Add slider to the figure\n",
    "fig.update_layout(\n",
    "    sliders=[{\n",
    "        'active': 0,\n",
    "        'pad': {\"t\": 50},\n",
    "        'steps': slider_steps\n",
    "    }],\n",
    "    title=' vs SalePrice (Excluding LotArea, SalePrice, and Id)',\n",
    "    xaxis_title='Feature Value',  # This will be updated dynamically\n",
    "    yaxis_title='SalePrice'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c298cf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Assuming df is your dataframe\n",
    "# Step 1: Sort out numeric features\n",
    "numeric_features = test_df.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "# Step 2: Exclude 'LotArea', 'SalePrice', and 'Id'\n",
    "numeric_features_excluded = numeric_features.drop(columns=['SalePrice', 'Id'], errors='ignore')\n",
    "\n",
    "# Step 3: Create an empty figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Step 4: Add a scatter plot for each feature and make them invisible initially\n",
    "for feature in numeric_features_excluded.columns:\n",
    "    fig.add_trace(go.Scatter(x=test_df[feature], y=train_df['SalePrice'], mode='markers', name=feature, visible=False))\n",
    "\n",
    "# Step 5: Make the first feature visible\n",
    "fig.data[0].visible = True\n",
    "\n",
    "# Step 6: Create slider steps\n",
    "slider_steps = [\n",
    "    {'label': feature, 'method': 'update', 'args': [{'visible': [i == j for j in range(len(fig.data))]}, {'title': f'{feature} vs SalePrice', 'xaxis': {'title': feature}}]}\n",
    "    for i, feature in enumerate(numeric_features_excluded.columns)\n",
    "]\n",
    "\n",
    "# Step 7: Add slider to the figure\n",
    "fig.update_layout(\n",
    "    sliders=[{\n",
    "        'active': 0,\n",
    "        'pad': {\"t\": 50},\n",
    "        'steps': slider_steps\n",
    "    }],\n",
    "    title=' vs SalePrice (Excluding LotArea, SalePrice, and Id)',\n",
    "    xaxis_title='Feature Value',  # This will be updated dynamically\n",
    "    yaxis_title='SalePrice'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc461e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outliers\n",
    "# LotFrontage >300\n",
    "# LotArea > 100000\n",
    "# MasVnrArea > 1500\n",
    "# BsmtFinSF1 > 5000\n",
    "# TotalBsmtSF > 6000\n",
    "# 1stFlrSF > 4500\n",
    "# GrLivArea > 5500\n",
    "# OpenPorchSF > 500\n",
    "# EnclosedPorch > 500\n",
    "# 3SsnPorch > 400\n",
    "# ScreenPorch > 400\n",
    "# MiscVal > 80000\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "def find_outliers_for_all_numeric_features(df, multiplier=3):\n",
    "    # Step 1: Select only the numeric features\n",
    "    numeric_features = df.select_dtypes(include=['int64', 'float64'])\n",
    "    \n",
    "    # Step 2: Exclude specific columns\n",
    "    columns_to_exclude = ['Id', 'MSSubClass', 'OverallQual', 'OverallCond']\n",
    "    numeric_features = numeric_features.drop(columns=columns_to_exclude, errors='ignore')\n",
    "    \n",
    "    # Step 3: Create a dictionary to store outliers DataFrames\n",
    "    outliers_dict = {}\n",
    "    \n",
    "    # Step 4: Iterate over each numeric feature and find outlier rows using IQR method\n",
    "    for feature in numeric_features.columns:\n",
    "        # Step 5: Calculate Q1 (25th percentile) and Q3 (75th percentile)\n",
    "        Q1 = df[feature].quantile(0.03)\n",
    "        Q3 = df[feature].quantile(0.97)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        # Step 6: Define the lower and upper bounds for outliers with a more stringent multiplier\n",
    "        lower_bound = Q1 - multiplier * IQR\n",
    "        upper_bound = Q3 + multiplier * IQR\n",
    "        \n",
    "        # Step 7: Find the rows where the value is outside the bounds\n",
    "        outliers = df[(df[feature] < lower_bound) | (df[feature] > upper_bound)]\n",
    "        \n",
    "        # Step 8: Sort the outliers DataFrame by the feature in descending order\n",
    "        outliers_sorted = outliers.sort_values(by=feature, ascending=False)\n",
    "        \n",
    "        # Step 9: Store the sorted outliers DataFrame in the dictionary\n",
    "        outliers_dict[feature] = outliers_sorted\n",
    "    \n",
    "    return outliers_dict\n",
    "\n",
    "outliers_dict = find_outliers_for_all_numeric_features(train_df, multiplier=1.5)\n",
    "\n",
    "# Create a dropdown widget\n",
    "dropdown = widgets.Dropdown(\n",
    "    options=outliers_dict.keys(),\n",
    "    description='Feature:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "# Function to display the selected DataFrame\n",
    "def display_outliers(change):\n",
    "    feature = change['new']\n",
    "    clear_output(wait=True)\n",
    "    display(dropdown)\n",
    "    # Highlight the selected feature\n",
    "    styled_df = outliers_dict[feature].style.applymap(lambda x: 'background-color: yellow', subset=[feature])\n",
    "    display(styled_df)\n",
    "\n",
    "# Set up the event listener for the dropdown\n",
    "dropdown.observe(display_outliers, names='value')\n",
    "\n",
    "# Display the dropdown\n",
    "display(dropdown)\n",
    "\n",
    "# Display the initial DataFrame with highlighting\n",
    "initial_feature = next(iter(outliers_dict))\n",
    "styled_initial_df = outliers_dict[initial_feature].style.applymap(lambda x: 'background-color: yellow', subset=[initial_feature])\n",
    "display(styled_initial_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf40518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Outliers Based on Defined Criteria\n",
    "\n",
    "# Define outlier conditions\n",
    "outlier_conditions = (\n",
    "    (train_df['LotFrontage'] > 300) |\n",
    "    (train_df['LotArea'] > 100000) |\n",
    "    (train_df['MasVnrArea'] > 1500) |\n",
    "    (train_df['BsmtFinSF1'] > 5000) |\n",
    "    (train_df['TotalBsmtSF'] > 6000) |\n",
    "    (train_df['1stFlrSF'] > 4500) |\n",
    "    (train_df['GrLivArea'] > 5500) |\n",
    "    (train_df['OpenPorchSF'] > 500) |\n",
    "    (train_df['EnclosedPorch'] > 500) |\n",
    "    (train_df['3SsnPorch'] > 400) |\n",
    "    (train_df['ScreenPorch'] > 400) |\n",
    "    (train_df['MiscVal'] > 80000)\n",
    ")\n",
    "\n",
    "# Display number of outliers detected\n",
    "num_outliers = train_df[outlier_conditions].shape[0]\n",
    "print(f\"Number of outliers detected: {num_outliers}\")\n",
    "\n",
    "# Remove outliers from the training dataset\n",
    "train_df_cleaned = train_df[~outlier_conditions].reset_index(drop=True)\n",
    "\n",
    "# Display the new shape of the training dataset\n",
    "print(f\"New training dataset size: {train_df_cleaned.shape}\")\n",
    "\n",
    "# (Optional) Save the cleaned dataset for future use\n",
    "# train_df_cleaned.to_csv('/path/to/save/cleaned_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029d2c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Outliers Based on Defined Criteria\n",
    "\n",
    "# Define outlier conditions\n",
    "outlier_conditions = (\n",
    "    (test_df['LotFrontage'] > 300) |\n",
    "    (test_df['LotArea'] > 100000) |\n",
    "    (test_df['MasVnrArea'] > 1500) |\n",
    "    (test_df['BsmtFinSF1'] > 5000) |\n",
    "    (test_df['TotalBsmtSF'] > 6000) |\n",
    "    (test_df['1stFlrSF'] > 4500) |\n",
    "    (test_df['GrLivArea'] > 5500) |\n",
    "    (test_df['OpenPorchSF'] > 500) |\n",
    "    (test_df['EnclosedPorch'] > 500) |\n",
    "    (test_df['3SsnPorch'] > 400) |\n",
    "    (test_df['ScreenPorch'] > 400) |\n",
    "    (test_df['GarageYrBlt'] > 2020) |\n",
    "    (test_df['MiscVal'] > 80000)\n",
    ")\n",
    "\n",
    "# Display number of outliers detected\n",
    "num_outliers = test_df[outlier_conditions].shape[0]\n",
    "print(f\"Number of outliers detected: {num_outliers}\")\n",
    "\n",
    "# Remove outliers from the training dataset\n",
    "test_df_cleaned = test_df[~outlier_conditions].reset_index(drop=True)\n",
    "\n",
    "# Display the new shape of the training dataset\n",
    "print(f\"New training dataset size: {test_df_cleaned.shape}\")\n",
    "\n",
    "# (Optional) Save the cleaned dataset for future use\n",
    "# train_df_cleaned.to_csv('/path/to/save/cleaned_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6164c4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(train_df_cleaned.isnull().sum().sort_values(ascending=False)).head(20)\n",
    "\n",
    "pd.DataFrame(test_df_cleaned.isnull().sum().sort_values(ascending=False)).head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81681987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import pandas as pd\n",
    "\n",
    "# Identify categorical features\n",
    "categorical_features = train_df_cleaned.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Calculate null counts for categorical features\n",
    "null_counts_dict = {\n",
    "    feature: train_df_cleaned[feature].isnull().sum()\n",
    "    for feature in categorical_features\n",
    "}\n",
    "\n",
    "# Sort categorical features by null counts\n",
    "sorted_categorical_features = sorted(categorical_features, key=lambda x: null_counts_dict[x], reverse=True)\n",
    "\n",
    "# Create a dictionary of unique values and their counts for each categorical feature\n",
    "unique_counts_dict = {\n",
    "    feature: train_df_cleaned[feature].value_counts(dropna=False).sort_values(ascending=False)\n",
    "    for feature in sorted_categorical_features\n",
    "}\n",
    "\n",
    "# Create a dropdown widget for selecting categorical features\n",
    "categorical_dropdown_counts = widgets.Dropdown(\n",
    "    options=sorted_categorical_features,\n",
    "    description='Feature:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "# Function to display unique values and their counts based on selected feature\n",
    "def display_unique_counts(change):\n",
    "    feature = change['new']\n",
    "    clear_output(wait=True)\n",
    "    display(categorical_dropdown_counts)\n",
    "    \n",
    "    counts_series = unique_counts_dict.get(feature, pd.Series(dtype=int))\n",
    "    \n",
    "    # Replace NaN with 'NaN' string for better readability\n",
    "    counts_series = counts_series.rename(index={pd.NA: 'NaN', None: 'NaN'})\n",
    "    counts_series.index = counts_series.index.fillna('NaN')\n",
    "    \n",
    "    # Create a DataFrame for better display\n",
    "    counts_df = counts_series.reset_index()\n",
    "    counts_df.columns = [feature, 'Count']\n",
    "    \n",
    "    # Display the counts DataFrame\n",
    "    display(counts_df)\n",
    "\n",
    "# Set up the observer for the dropdown menu\n",
    "categorical_dropdown_counts.observe(display_unique_counts, names='value')\n",
    "\n",
    "# Display the dropdown menu\n",
    "display(categorical_dropdown_counts)\n",
    "\n",
    "# Display unique counts for the initially selected feature\n",
    "if sorted_categorical_features:\n",
    "    display_unique_counts({'new': categorical_dropdown_counts.value})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52308efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import pandas as pd\n",
    "\n",
    "# Identify categorical features\n",
    "categorical_features = test_df_cleaned.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Calculate null counts for categorical features\n",
    "null_counts_dict = {\n",
    "    feature: test_df_cleaned[feature].isnull().sum()\n",
    "    for feature in categorical_features\n",
    "}\n",
    "\n",
    "# Sort categorical features by null counts\n",
    "sorted_categorical_features = sorted(categorical_features, key=lambda x: null_counts_dict[x], reverse=True)\n",
    "\n",
    "# Create a dictionary of unique values and their counts for each categorical feature\n",
    "unique_counts_dict = {\n",
    "    feature: test_df_cleaned[feature].value_counts(dropna=False).sort_values(ascending=False)\n",
    "    for feature in sorted_categorical_features\n",
    "}\n",
    "\n",
    "# Create a dropdown widget for selecting categorical features\n",
    "categorical_dropdown_counts = widgets.Dropdown(\n",
    "    options=sorted_categorical_features,\n",
    "    description='Feature:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "# Function to display unique values and their counts based on selected feature\n",
    "def display_unique_counts(change):\n",
    "    feature = change['new']\n",
    "    clear_output(wait=True)\n",
    "    display(categorical_dropdown_counts)\n",
    "    \n",
    "    counts_series = unique_counts_dict.get(feature, pd.Series(dtype=int))\n",
    "    \n",
    "    # Replace NaN with 'NaN' string for better readability\n",
    "    counts_series = counts_series.rename(index={pd.NA: 'NaN', None: 'NaN'})\n",
    "    counts_series.index = counts_series.index.fillna('NaN')\n",
    "    \n",
    "    # Create a DataFrame for better display\n",
    "    counts_df = counts_series.reset_index()\n",
    "    counts_df.columns = [feature, 'Count']\n",
    "    \n",
    "    # Display the counts DataFrame\n",
    "    display(counts_df)\n",
    "\n",
    "# Set up the observer for the dropdown menu\n",
    "categorical_dropdown_counts.observe(display_unique_counts, names='value')\n",
    "\n",
    "# Display the dropdown menu\n",
    "display(categorical_dropdown_counts)\n",
    "\n",
    "# Display unique counts for the initially selected feature\n",
    "if sorted_categorical_features:\n",
    "    display_unique_counts({'new': categorical_dropdown_counts.value})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cfd464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the order for each ordinal categorical variable with '0' for NaN\n",
    "ordinal_mappings = {\n",
    "    'ExterQual': {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5, np.nan: 0},\n",
    "    'ExterCond': {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5, np.nan: 0},\n",
    "    'BsmtQual': {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5, np.nan: 0},\n",
    "    'BsmtCond': {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5, np.nan: 0},\n",
    "    'BsmtExposure': {'No': 1, 'Mn': 2, 'Av': 3, 'Gd': 4, np.nan: 0},\n",
    "    'BsmtFinType1': {'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6, np.nan: 0},\n",
    "    'BsmtFinType2': {'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6, np.nan: 0},\n",
    "    'HeatingQC': {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5, np.nan: 0},\n",
    "    'KitchenQual': {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5, np.nan: 0},\n",
    "    'Functional': {\n",
    "        'Sal': 1,\n",
    "        'Sev': 2,\n",
    "        'Maj2': 3,\n",
    "        'Maj1': 4,\n",
    "        'Mod': 5,\n",
    "        'Min2': 6,\n",
    "        'Min1': 7,\n",
    "        'Typ': 8,\n",
    "        np.nan: 0\n",
    "    },\n",
    "    'FireplaceQu': {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5, np.nan: 0},\n",
    "    'GarageFinish': {'Unf': 1, 'RFn': 2, 'Fin': 3, np.nan: 0},\n",
    "    'GarageQual': {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5, np.nan: 0},\n",
    "    'GarageCond': {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5, np.nan: 0},\n",
    "    'PoolQC': {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5, np.nan: 0},\n",
    "    'Fence': {'MnWw': 1, 'MnPrv': 2, 'GdWo': 3, 'GdPrv': 4, np.nan: 0}\n",
    "}\n",
    "\n",
    "# Make a copy of the dataset to avoid modifying the original data\n",
    "df = train_df_cleaned.copy()\n",
    "# Encode each ordinal categorical variable and handle NaN values by assigning '0'\n",
    "for col, mapping in ordinal_mappings.items():\n",
    "    df[col] = df[col].map(mapping).fillna(0).astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa8609f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the order for each ordinal categorical variable with '0' for NaN\n",
    "ordinal_mappings = {\n",
    "    'ExterQual': {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5, np.nan: 0},\n",
    "    'ExterCond': {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5, np.nan: 0},\n",
    "    'BsmtQual': {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5, np.nan: 0},\n",
    "    'BsmtCond': {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5, np.nan: 0},\n",
    "    'BsmtExposure': {'No': 1, 'Mn': 2, 'Av': 3, 'Gd': 4, np.nan: 0},\n",
    "    'BsmtFinType1': {'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6, np.nan: 0},\n",
    "    'BsmtFinType2': {'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6, np.nan: 0},\n",
    "    'HeatingQC': {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5, np.nan: 0},\n",
    "    'KitchenQual': {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5, np.nan: 0},\n",
    "    'Functional': {\n",
    "        'Sal': 1,\n",
    "        'Sev': 2,\n",
    "        'Maj2': 3,\n",
    "        'Maj1': 4,\n",
    "        'Mod': 5,\n",
    "        'Min2': 6,\n",
    "        'Min1': 7,\n",
    "        'Typ': 8,\n",
    "        np.nan: 0\n",
    "    },\n",
    "    'FireplaceQu': {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5, np.nan: 0},\n",
    "    'GarageFinish': {'Unf': 1, 'RFn': 2, 'Fin': 3, np.nan: 0},\n",
    "    'GarageQual': {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5, np.nan: 0},\n",
    "    'GarageCond': {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5, np.nan: 0},\n",
    "    'PoolQC': {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5, np.nan: 0},\n",
    "    'Fence': {'MnWw': 1, 'MnPrv': 2, 'GdWo': 3, 'GdPrv': 4, np.nan: 0}\n",
    "}\n",
    "\n",
    "# Make a copy of the dataset to avoid modifying the original data\n",
    "df = test_df_cleaned.copy()\n",
    "# Encode each ordinal categorical variable and handle NaN values by assigning '0'\n",
    "for col, mapping in ordinal_mappings.items():\n",
    "    df[col] = df[col].map(mapping).fillna(0).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "# List of nominal categorical variables\n",
    "nominal_vars = [\n",
    "    'MSSubClass', 'MSZoning', 'Street', 'Alley', 'LotShape',\n",
    "    'LandContour', 'Utilities', 'LotConfig', 'LandSlope',\n",
    "    'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n",
    "    'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st',\n",
    "    'Exterior2nd', 'Foundation', 'Heating', 'CentralAir',\n",
    "    'Electrical', 'SaleType', 'SaleCondition', 'PavedDrive',\n",
    "    'MiscFeature','MasVnrType','GarageType']\n",
    "\n",
    "# Fill NaN values in nominal variables with 'None' before one-hot encoding\n",
    "df[nominal_vars] = df[nominal_vars].fillna('None')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0135aa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of nominal categorical variables\n",
    "nominal_vars = [\n",
    "    'MSSubClass', 'MSZoning', 'Street', 'Alley', 'LotShape',\n",
    "    'LandContour', 'Utilities', 'LotConfig', 'LandSlope',\n",
    "    'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n",
    "    'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st',\n",
    "    'Exterior2nd', 'Foundation', 'Heating', 'CentralAir',\n",
    "    'Electrical', 'SaleType', 'SaleCondition', 'PavedDrive',\n",
    "    'MiscFeature','MasVnrType','GarageType']\n",
    "\n",
    "# Fill NaN values in nominal variables with 'None' before one-hot encoding\n",
    "df[nominal_vars] = df[nominal_vars].fillna('None')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11175793",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "# One-hot encode nominal variables with drop_first to avoid dummy variable trap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdca987b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode nominal variables with drop_first to avoid dummy variable trap\n",
    "df = pd.get_dummies(df, columns=nominal_vars, drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505014d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "# One-hot encode nominal variables with drop_first to avoid dummy variable trap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b1c4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify remaining categorical columns not yet encoded\n",
    "remaining_categorical = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# If there are any remaining categorical columns, encode them\n",
    "if remaining_categorical:\n",
    "    print(\"Encoding the following remaining categorical columns:\")\n",
    "    for col in remaining_categorical:\n",
    "        print(f\"- {col}\")\n",
    "        \n",
    "        # Ensure 'None' is a category before filling NaN values\n",
    "        if pd.api.types.is_categorical_dtype(df[col]):\n",
    "            df[col] = df[col].cat.add_categories(['None'])\n",
    "        \n",
    "    # Fill NaN values with 'None' to handle missing data\n",
    "    df[remaining_categorical] = df[remaining_categorical].fillna('None')\n",
    "    \n",
    "    # One-hot encode the remaining categorical variables\n",
    "    df = pd.get_dummies(df, columns=remaining_categorical, drop_first=True)\n",
    "else:\n",
    "    print(\"No remaining categorical columns to encode.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ec0214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified Code to Display Unique Values of Encoded Categorical Variables\n",
    "\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Function to extract original categorical features and their unique categories from encoded DataFrame\n",
    "def get_encoded_categories(df, nominal_vars):\n",
    "    \"\"\"\n",
    "    Extracts the original categorical features and their unique categories based on one-hot encoded columns.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The encoded DataFrame.\n",
    "    - nominal_vars (list): List of nominal categorical variables that were one-hot encoded.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: A dictionary where keys are original features and values are lists of categories.\n",
    "    \"\"\"\n",
    "    encoded_categories = {}\n",
    "    for var in nominal_vars:\n",
    "        # Find all columns that start with the nominal variable name followed by an underscore\n",
    "        pattern = f\"{var}_\"\n",
    "        matching_cols = [col for col in df.columns if col.startswith(pattern)]\n",
    "        # Extract the category names by removing the variable prefix\n",
    "        categories = [col[len(pattern):] for col in matching_cols]\n",
    "        encoded_categories[var] = categories\n",
    "    return encoded_categories\n",
    "\n",
    "# List of nominal variables that were one-hot encoded\n",
    "nominal_vars = [\n",
    "    'MSSubClass', 'MSZoning', 'Street', 'Alley', 'LotShape',\n",
    "    'LandContour', 'Utilities', 'LotConfig', 'LandSlope',\n",
    "    'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n",
    "    'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st',\n",
    "    'Exterior2nd', 'Foundation', 'Heating', 'CentralAir',\n",
    "    'Electrical', 'SaleType', 'SaleCondition', 'PavedDrive',\n",
    "    'MiscFeature','MasVnrType','GarageType','house_age','mkt_cyc'\n",
    "]\n",
    "\n",
    "# Get the mapping of original categorical features to their categories\n",
    "encoded_categories = get_encoded_categories(df, nominal_vars)\n",
    "\n",
    "# Display the encoded categories using a dropdown\n",
    "encoded_dropdown = widgets.Dropdown(\n",
    "    options=encoded_categories.keys(),\n",
    "    description='Feature:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "# Function to display categories for the selected feature\n",
    "def display_encoded_categories(change):\n",
    "    feature = change['new']\n",
    "    clear_output(wait=True)\n",
    "    display(encoded_dropdown)\n",
    "    \n",
    "    categories = encoded_categories.get(feature, [])\n",
    "    categories_df = pd.DataFrame({\n",
    "        'Encoded Column': [f\"{feature}_{cat}\" for cat in categories],\n",
    "        'Original Category': categories\n",
    "    })\n",
    "    \n",
    "    display(categories_df)\n",
    "\n",
    "# Set up the observer for the dropdown menu\n",
    "encoded_dropdown.observe(display_encoded_categories, names='value')\n",
    "\n",
    "# Display the dropdown menu\n",
    "display(encoded_dropdown)\n",
    "\n",
    "# Display unique categories for the initially selected feature\n",
    "if encoded_categories:\n",
    "    initial_feature = list(encoded_categories.keys())[0]\n",
    "    display_encoded_categories({'new': initial_feature})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c85645b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "train_df=df\n",
    "# Compute the correlation matrix\n",
    "\n",
    "\n",
    "# Step 1: Compute the correlation matrix\n",
    "correlation_matrix = train_df.corr()\n",
    "\n",
    "# Step 2: Set the threshold for high correlation\n",
    "threshold = 0.8\n",
    "\n",
    "# Step 3: Filter the correlation matrix for values above the threshold\n",
    "# We take the absolute value to capture both positive and negative correlations\n",
    "high_corr_matrix = correlation_matrix[(correlation_matrix.abs() > threshold) & (correlation_matrix.abs() < 1)]\n",
    "\n",
    "# Step 4: Drop rows and columns with all NaN values (since the matrix will be sparse after filtering)\n",
    "high_corr_matrix = high_corr_matrix.dropna(how='all').dropna(axis=1, how='all')\n",
    "\n",
    "# Step 5: Visualize the filtered correlation matrix\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(high_corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', cbar=True)\n",
    "plt.title('Highly Correlated Features (|Correlation| > 0.8)')\n",
    "plt.show()\n",
    "\n",
    "# Step 6: (Optional) List the pairs of features with high correlation\n",
    "# Extract the pairs of features and their correlation values\n",
    "high_corr_pairs = high_corr_matrix.stack().reset_index()\n",
    "high_corr_pairs.columns = ['Feature1', 'Feature2', 'Correlation']\n",
    "high_corr_pairs = high_corr_pairs.sort_values(by='Correlation', ascending=False)\n",
    "\n",
    "# Display the highly correlated pairs\n",
    "print(high_corr_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9973efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "def calculate_vif(df):\n",
    "    \"\"\"\n",
    "    Calculate Variance Inflation Factor (VIF) for each feature in the dataframe.\n",
    "    \"\"\"\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data['Feature'] = df.columns\n",
    "    vif_data['VIF'] = [\n",
    "        variance_inflation_factor(df.values, i) for i in range(df.shape[1])\n",
    "    ]\n",
    "    return vif_data.sort_values(by='VIF', ascending=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0367c1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target variable\n",
    "y_train= train_df['SalePrice']\n",
    "y_train.to_csv('/Users/issacsmacbookpro/.cursor-tutor/projects/Housing Price Prediction/house-prices-advanced-regression-techniques/y_train.csv')\n",
    "\n",
    "# Drop the target and aggregated variables from features\n",
    "X_train= train_df.drop(columns=['SalePrice', 'TotalBsmtSF', 'GrLivArea','BldgType_Duplex','Exterior1st_CBlock','Exterior2nd_CBlock', 'MSSubClass_90'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2827b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "vif = calculate_vif(train_df)\n",
    "# Display VIF values\n",
    "print(vif.sort_values(by='VIF', ascending=False))\n",
    "# Export VIF values to CSV\n",
    "vif.sort_values(by='VIF', ascending=False).to_csv('vif_values.csv', index=False)  # Specify the path if needed\n",
    "\n",
    "# Export VIF values to CSV\n",
    "vif.to_csv('vif_values.csv', index=False)  # Specify the path if needed\n",
    "\n",
    "if 'constant' in X_train.columns:\n",
    "    X_train = X_train.drop(columns=['constant'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a15885b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "def remove_high_vif_features(df, threshold=10, exclude_features=None):\n",
    "    \"\"\"\n",
    "    Iteratively remove features with VIF > threshold, excluding specified features.\n",
    "    \"\"\"\n",
    "    if exclude_features is None:\n",
    "        exclude_features = []\n",
    "        \n",
    "    iteration = 1\n",
    "    while True:\n",
    "        vif = calculate_vif(df)\n",
    "        # Sort VIF DataFrame by VIF values in descending order\n",
    "        vif = vif.sort_values('VIF', ascending=False).reset_index(drop=True)\n",
    "        \n",
    "        # Find the feature with the highest VIF not in exclude_features\n",
    "        for idx, row in vif.iterrows():\n",
    "            if row['Feature'] not in exclude_features:\n",
    "                max_vif = row['VIF']\n",
    "                feature_to_remove = row['Feature']\n",
    "                break\n",
    "        else:\n",
    "            # All features are in exclude_features\n",
    "            break\n",
    "        \n",
    "        print(f\"Iteration {iteration}:\")\n",
    "        print(vif.head(10))\n",
    "        print(f\"Max VIF: {max_vif} (Feature to remove: {feature_to_remove})\\n\")\n",
    "        \n",
    "        if max_vif > threshold:\n",
    "            print(f\"Removing '{feature_to_remove}' with VIF: {max_vif}\\n\")\n",
    "            df = df.drop(columns=[feature_to_remove])\n",
    "            iteration += 1\n",
    "        else:\n",
    "            print(\"All VIF values are below the threshold or only excluded features have high VIF.\\n\")\n",
    "            break\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3358fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove exact duplicates\n",
    "duplicate_columns = X_train.columns[X_train.columns.duplicated()]\n",
    "if len(duplicate_columns) > 0:\n",
    "    print(f\"Duplicate columns found: {duplicate_columns.tolist()}\")\n",
    "    X_train = X_train.loc[:, ~X_train.columns.duplicated()]\n",
    "else:\n",
    "    print(\"No duplicate columns found.\")\n",
    "\n",
    "# Remove near-duplicates based on high correlation\n",
    "corr_matrix = X_train.corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.9999)]\n",
    "if to_drop:\n",
    "    print(f\"Removing near-duplicate columns: {to_drop}\")\n",
    "    X_train = X_train.drop(columns=to_drop)\n",
    "else:\n",
    "    print(\"No near-duplicate columns found.\")\n",
    "\n",
    "# If not already done, ensure one-hot encoding with drop_first=True\n",
    "X_train = pd.get_dummies(X_train, drop_first=True)\n",
    "print(f\"After one-hot encoding, number of features: {X_train.shape[1]}\")\n",
    "\n",
    "# Use the correct column names in exclude_features\n",
    "X_train_reduced = remove_high_vif_features(X_train, threshold=10, exclude_features=['YrSold','house_age'])\n",
    "print(f\"Shape after VIF reduction: {X_train_reduced.shape}\")\n",
    "\n",
    "\n",
    "final_vif = calculate_vif(X_train_reduced)\n",
    "print(\"Final VIF Scores:\\n\", final_vif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b272afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the columns of the original and reduced DataFrames\n",
    "original_columns = X_train.columns\n",
    "reduced_columns = X_train_reduced.columns\n",
    "\n",
    "# Find removed columns\n",
    "removed_columns = set(original_columns) - set(reduced_columns)\n",
    "\n",
    "# Display the removed columns\n",
    "print(\"Columns removed from X_train_numeric to X_train_reduced:\")\n",
    "print(removed_columns)\n",
    "X_train_reduced.to_csv('Users/issacsmacbookpro/.cursor-tutor/projects/Housing Price Prediction/house-prices-advanced-regression-techniques/X_train_reduced.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29217913",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=pd.read_csv('/Users/issacsmacbookpro/.cursor-tutor/projects/Housing Price Prediction/house-prices-advanced-regression-techniques/X_train_reduced.csv')\n",
    "y_train=pd.read_csv('/Users/issacsmacbookpro/.cursor-tutor/projects/Housing Price Prediction/house-prices-advanced-regression-techniques/y_train.csv')\n",
    "test_df = pd.read_csv('/Users/issacsmacbookpro/.cursor-tutor/projects/Housing Price Prediction/house-prices-advanced-regression-techniques/test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db41b15f",
   "metadata": {},
   "source": [
    "## Using train_test_split to split the data into training and validation sets (80% train, 20% validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf814975",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and validation sets (80% train, 20% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b2c65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Initialize the XGBoost Regressor\n",
    "xgboost_model = xgb.XGBRegressor(\n",
    "    n_estimators=1000,   # Number of trees (boosting rounds)\n",
    "    learning_rate=0.05,  # Learning rate\n",
    "    max_depth=6,         # Maximum depth of each tree\n",
    "    subsample=0.8,       # Subsample ratio of the training instance\n",
    "    colsample_bytree=0.8,  # Subsample ratio of columns when constructing each tree\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgboost_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],  # Validation set for early stopping\n",
    "    early_stopping_rounds=50,   # Stop if validation performance does not improve for 50 rounds\n",
    "    verbose=True  # Print training progress\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6df47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the validation set\n",
    "y_val_pred = xgboost_model.predict(X_val)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = mean_squared_error(y_val, y_val_pred, squared=False)\n",
    "print(f\"Validation RMSE: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363c51d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# import numpy as np\n",
    "\n",
    "# # 定义模型\n",
    "# models = {\n",
    "#     'XGBoost': xgb.XGBRegressor(),\n",
    "#     'LightGBM': lgb.LGBMRegressor(),\n",
    "#     'CatBoost': catboost.CatBoostRegressor(),\n",
    "#     'Random Forest': RandomForestRegressor(),\n",
    "#     'Ridge Regression': Ridge(),\n",
    "#     'Lasso Regression': Lasso()\n",
    "# }\n",
    "\n",
    "# # 交叉验证对每个模型进行评估\n",
    "# for name, model in models.items():\n",
    "#     scores = cross_val_score(model, x_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "#     rmse_scores = np.sqrt(-scores)\n",
    "#     print(f\"{name}: Mean RMSE = {rmse_scores.mean():.4f}, Std RMSE = {rmse_scores.std():.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
